import os as os
import matplotlib.pyplot as plt
import cv2
print(f"âœ… OpenCV version: {cv2.__version__}")
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
import os
import argparse
from datetime import datetime
import argparse
import seaborn as sns
import sys 


# ============= FONCTION UTILITAIRE POUR VALIDATION CONFIG =============
def validate_config():
    """
    Valide la configuration avant le lancement
    """
    global model_to_use, variables, epochs, learning_rate, beta, MODELS
    
    # VÃ©rifications de base
    available_models = [m.upper() for m in MODELS if m.upper() in ["VAE", "VQVAE", "WAE-MMD"]]
    assert model_to_use.upper() in available_models, f"ModÃ¨le '{model_to_use}' non supportÃ©. Disponibles: {available_models}"
    assert epochs > 0, "Le nombre d'Ã©poques doit Ãªtre positif"
    assert learning_rate > 0, "Le learning rate doit Ãªtre positif"
    assert beta >= 0, "Beta doit Ãªtre positif ou nul"
    
    # VÃ©rifications spÃ©cifiques
    if model_to_use.upper() == "VAE" or model_to_use.upper() == "WAE-MMD":
        if progressive_beta:
            assert initial_beta <= beta, "initial_beta doit Ãªtre <= beta final"
    
    print(f" Configuration validÃ©e pour {model_to_use.upper()}")
    


# Cellule 1: DÃ©finir le mode d'exÃ©cution
# Choisissez l'un des modes suivants en dÃ©commentant la ligne correspondante

# Mode 1: EntraÃ®nement normal avec sauvegarde automatique
#mode = "training"

# Mode 2: Chargement d'un modÃ¨le existant pour Ã©valuation
# mode = "load_and_evaluate"
# model_path = "results_vae_20240315_143022/ocean_vae_model.pth"

# Mode 3: InfÃ©rence rapide uniquement
# mode = "inference_only"
# model_path = "results_vae_20240315_143022/latest_model.pth"


# ============= FONCTIONS UTILITAIRES  =============

def quick_inference(model_path, test_data=None):
    """
    Fonction rapide pour charger un modÃ¨le et faire de l'infÃ©rence
    Parfait pour Jupyter Notebook
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model, stats_dict, checkpoint = load_model_universal(model_path, device)
    
    print(f" ModÃ¨le {checkpoint['model_type']} chargÃ©")
    print(f" Variables: {checkpoint.get('variables', [])}")
    
    if test_data is not None:
        print(" InfÃ©rence en cours...")
        model.eval()
        with torch.no_grad():
            if checkpoint['model_type'] in ["VAE", "WAE-MMD"]:
                reconstructed, mu, logvar = model(test_data.to(device))
                return model, reconstructed, mu, logvar
            elif checkpoint['model_type'] == "VQVAE":
                reconstructed, vq_loss, perplexity, encoding_indices = model(test_data.to(device))
                return model, reconstructed, vq_loss, perplexity, encoding_indices
    
    return model, stats_dict

def list_saved_models(base_dir="./"):
    """
    Liste tous les modÃ¨les sauvegardÃ©s dans le rÃ©pertoire
    """
    import glob
    
    model_files = glob.glob(os.path.join(base_dir, "**/ocean_*_model.pth"), recursive=True)
    latest_files = glob.glob(os.path.join(base_dir, "**/latest_model.pth"), recursive=True)
    
    print(" ModÃ¨les sauvegardÃ©s trouvÃ©s:")
    print("\n ModÃ¨les spÃ©cifiques:")
    for i, model_file in enumerate(model_files, 1):
        try:
            checkpoint = torch.load(model_file, map_location='cpu')
            model_type = checkpoint.get('model_type', 'Unknown')
            timestamp = checkpoint.get('timestamp', 'Unknown')
            print(f"   {i}. {model_file}")
            print(f"      Type: {model_type} | Timestamp: {timestamp}")
        except:
            print(f"   {i}. {model_file} ( Erreur de lecture)")
    
    print("\n ModÃ¨les 'latest':")
    for i, latest_file in enumerate(latest_files, 1):
        try:
            checkpoint = torch.load(latest_file, map_location='cpu')
            model_type = checkpoint.get('model_type', 'Unknown')
            timestamp = checkpoint.get('timestamp', 'Unknown')
            print(f"   {i}. {latest_file}")
            print(f"      Type: {model_type} | Timestamp: {timestamp}")
        except:
            print(f"   {i}. {latest_file} ( Erreur de lecture)")

def compare_models(model_paths):
    """
    Compare plusieurs modÃ¨les sauvegardÃ©s
    """
    print(" Comparaison des modÃ¨les:")
    
    for i, model_path in enumerate(model_paths, 1):
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            print(f"\n ModÃ¨le {i}: {os.path.basename(model_path)}")
            print(f"   Type: {checkpoint.get('model_type', 'Unknown')}")
            print(f"   Variables: {checkpoint.get('variables', [])}")
            print(f"   Timestamp: {checkpoint.get('timestamp', 'Unknown')}")
            
            if 'best_val_loss' in checkpoint:
                print(f"   Meilleure val loss: {checkpoint['best_val_loss']:.6f}")
            
            config = checkpoint.get('config', {})
            print(f"   Config: lr={config.get('learning_rate', 'N/A')}, "
                  f"beta={config.get('beta', 'N/A')}, "
                  f"epochs={config.get('epochs', 'N/A')}")
                  
        except Exception as e:
            print(f" Erreur lors du chargement de {model_path}: {e}")


# =============  INFÃ‰RENCE SUR NOUVELLES DONNÃ‰ES =============

def run_inference_on_new_data(model_path, new_data_tensor, mask_tensor=None):
    """
    Fonction complÃ¨te pour faire de l'infÃ©rence sur de nouvelles donnÃ©es
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model, stats_dict, checkpoint = load_model_universal(model_path, device)
    
    print(f"ðŸ” InfÃ©rence avec modÃ¨le {checkpoint['model_type']}")
    
    # PrÃ©paration des donnÃ©es
    new_data_tensor = new_data_tensor.to(device)
    if mask_tensor is not None:
        mask_tensor = mask_tensor.to(device)
    
    model.eval()
    with torch.no_grad():
        if checkpoint['model_type'] in ["VAE", "WAE-MMD"]:
            if mask_tensor is not None:
                reconstructed, mu, logvar = model(new_data_tensor, mask_tensor)
            else:
                reconstructed, mu, logvar = model(new_data_tensor)
            
            print(f"âœ… Reconstruction terminÃ©e")
            print(f"   Shape originale: {new_data_tensor.shape}")
            print(f"   Shape reconstruite: {reconstructed.shape}")
            print(f"   Shape latente: {mu.shape}")
            
            return reconstructed, mu, logvar
            
        elif checkpoint['model_type'] == "VQVAE":
            if mask_tensor is not None:
                reconstructed, vq_loss, perplexity, encoding_indices = model(new_data_tensor, mask_tensor)
            else:
                reconstructed, vq_loss, perplexity, encoding_indices = model(new_data_tensor)
            
            print(f"âœ… Reconstruction terminÃ©e")
            print(f"   Shape originale: {new_data_tensor.shape}")
            print(f"   Shape reconstruite: {reconstructed.shape}")
            print(f"   PerplexitÃ©: {perplexity.item():.2f}")
            print(f"   Codes utilisÃ©s: {len(torch.unique(encoding_indices))}")
            
            return reconstructed, vq_loss, perplexity, encoding_indices


# ============= VISUALISATION RAPIDE =============

def quick_visualization(model_path, test_data, test_mask=None, variable_names=None):
    """
    Visualisation rapide des rÃ©sultats d'infÃ©rence
    """
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model, stats_dict, checkpoint = load_model_universal(model_path, device)
    
    # InfÃ©rence
    model.eval()
    with torch.no_grad():
        if checkpoint['model_type'] in ["VAE", "WAE-MMD"]:
            reconstructed, mu, logvar = model(test_data.to(device))
        elif checkpoint['model_type'] == "VQVAE":
            reconstructed, vq_loss, perplexity, encoding_indices = model(test_data.to(device))
    
    # Visualisation
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # Original
    axes[0, 0].imshow(test_data[0, 0].cpu().numpy(), cmap='viridis')
    axes[0, 0].set_title('Original - Variable 1')
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(test_data[0, 1].cpu().numpy(), cmap='viridis')
    axes[0, 1].set_title('Original - Variable 2')
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(test_data[0, 2].cpu().numpy(), cmap='viridis')
    axes[0, 2].set_title('Original - Variable 3')
    axes[0, 2].axis('off')
    
    # Reconstruit
    axes[1, 0].imshow(reconstructed[0, 0].cpu().numpy(), cmap='viridis')
    axes[1, 0].set_title('Reconstruit - Variable 1')
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(reconstructed[0, 1].cpu().numpy(), cmap='viridis')
    axes[1, 1].set_title('Reconstruit - Variable 2')
    axes[1, 1].axis('off')
    
    axes[1, 2].imshow(reconstructed[0, 2].cpu().numpy(), cmap='viridis')
    axes[1, 2].set_title('Reconstruit - Variable 3')
    axes[1, 2].axis('off')
    
    plt.suptitle(f'RÃ©sultats {checkpoint["model_type"]}', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    return reconstructed

print(" Toutes les fonctions utilitaires sont prÃªtes!")
print(" Adaptez les chemins et variables selon vos besoins")